{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnign from torch: Cuda\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"\", category=Warning, module=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤— Questions and Answers Generator ðŸ\n",
    "### Generating Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Prayson W. Daniel\n",
      "\n",
      "Last updated: 2020-12-15T09:20:07.617086+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.6\n",
      "IPython version      : 7.19.0\n",
      "\n",
      "matplotlib  : 3.3.3\n",
      "numpy       : 1.19.4\n",
      "torch       : 1.7.1\n",
      "transformers: 4.0.1\n",
      "\n",
      "Compiler    : GCC 8.3.0\n",
      "OS          : Linux\n",
      "Release     : 4.19.128-microsoft-standard\n",
      "Machine     : x86_64\n",
      "Processor   : \n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -uniz -a \"Prayson W. Daniel\" -vm -p matplotlib,numpy,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from pathlib import Path\n",
    "from transformers import PreTrainedModel\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class ModelLoader:\n",
    "    \"\"\"\n",
    "    Downloading and Loading Hugging FaceModels \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name:str,\n",
    "                 model_directory:str,\n",
    "                 tokenizer_loader:PreTrainedTokenizer,\n",
    "                 model_loader:PreTrainedModel):\n",
    "        \n",
    "        self.model_name = Path(model_name)\n",
    "        self.model_directory = Path(model_directory)\n",
    "        self.model_loader = model_loader\n",
    "        self.tokenizer_loader = tokenizer_loader\n",
    "        \n",
    "        self.save_path = self.model_directory / self.model_name\n",
    "        \n",
    "        if not self.save_path.exists():\n",
    "            print(f'[+] {self.save_path} does not exit!')\n",
    "            self.save_path.mkdir(parents=True, exist_ok=True)\n",
    "            self.__download_model()         \n",
    "        \n",
    "        self.tokenizer, self.model = self.__load_model()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}(model={self.save_path})'\n",
    "        \n",
    "    # Download model from HuggingFace\n",
    "    def __download_model(self) -> None:\n",
    "\n",
    "        print(f'[+] Downloading {self.model_name}')\n",
    "        tokenizer = self.tokenizer_loader.from_pretrained(f'{self.model_name}')\n",
    "        model = self.model_loader.from_pretrained(f'{self.model_name}')\n",
    "\n",
    "        print(f'[+] Saving {self.model_name} to {self.save_path}')\n",
    "        tokenizer.save_pretrained(f'{self.save_path}')\n",
    "        model.save_pretrained(f'{self.save_path}')\n",
    "        \n",
    "        print('[+] Process completed')\n",
    "    \n",
    "    # Load model\n",
    "    def __load_model(self)->t.Tuple:\n",
    "    \n",
    "        print(f'[+] Loading model from {self.save_path}')\n",
    "        tokenizer = self.tokenizer_loader.from_pretrained(f'{self.save_path}')\n",
    "        model = self.model_loader.from_pretrained(f'{self.save_path}')\n",
    "\n",
    "        print(f'[+] Loading completed')\n",
    "        \n",
    "        return tokenizer, model\n",
    "    \n",
    "    def retrieve(self):\n",
    "        return self.tokenizer, self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading model from ml_model/mrm8488/t5-base-finetuned-question-generation-ap\n",
      "[+] Loading completed\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "MODEL_PATH = \"./ml_model\"\n",
    "\n",
    "model_loader = ModelLoader(model_name=MODEL_NAME,\n",
    "                            model_directory=MODEL_PATH,\n",
    "                            tokenizer_loader=AutoTokenizer,\n",
    "                            model_loader=AutoModelForSeq2SeqLM\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_loader.model\n",
    "tokenizer = model_loader.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(answer, context, max_length=64):\n",
    "    input_text = f\"answer: {answer}  context: {context}\"\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length)\n",
    "\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> question: How big is the snail in this tale?</s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = 'This is a tale of a tiny snail and a great big grey blue humpback whale'\n",
    "answer = \"tiny\"\n",
    "answer_question(answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> question: What color is the humpback whale?</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question('grey', context, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> question: What size is the humpback whale?</s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question('big', context, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading model from ml_model/deepset/roberta-base-squad2\n",
      "[+] Loading completed\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer, model = ModelLoader(model_name=\"deepset/roberta-base-squad2\",\n",
    "                               model_directory=MODEL_PATH,\n",
    "                               tokenizer_loader=AutoTokenizer,\n",
    "                               model_loader=AutoModelForQuestionAnswering\n",
    "                                ).retrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, context):\n",
    "    \n",
    "    inputs = tokenizer(question, context, \n",
    "                       add_special_tokens=True, \n",
    "                       return_tensors='pt')\n",
    "    \n",
    "    input_ids = inputs['input_ids'].tolist()[0]\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = outputs.start_logits.argmax().item()\n",
    "    answer_end = outputs.end_logits.argmax().item() + 1\n",
    "\n",
    "    answer = tokenizer.decode(input_ids[answer_start:answer_end])\n",
    "\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'This is a tale of a tiny snail and a great big grey blue humpback whale'\n",
    "\n",
    "q1 = \"What is the color of the humpback whale?\"\n",
    "q2 = \"What kind of a whale is it?\"\n",
    "\n",
    "output = ask_question(q2, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grey blue humpback'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grey blue\n",
      "0.34658950567245483\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(q1, context, \n",
    "                       add_special_tokens=True, \n",
    "                       return_tensors='pt')\n",
    "    \n",
    "input_ids = inputs['input_ids'].tolist()[0]\n",
    "outputs = model(**inputs)\n",
    "    \n",
    "\n",
    "\n",
    "start = outputs.start_logits.detach().numpy()\n",
    "end = outputs.end_logits.detach().numpy()\n",
    "\n",
    "# Ensure padded tokens & question tokens cannot belong to the set of candidate answers.\n",
    "#?? undesired_tokens = np.abs(np.array(feature.p_mask) - 1) & feature.attention_mask\n",
    "\n",
    "# Generate mask\n",
    "\n",
    "undesired_tokens = inputs['attention_mask']\n",
    "undesired_tokens_mask = undesired_tokens == 0.0\n",
    "\n",
    "# Make sure non-context indexes in the tensor cannot contribute to the softmax\n",
    "start_ = np.where(undesired_tokens_mask, -10000.0, start)\n",
    "end_ = np.where(undesired_tokens_mask, -10000.0, end)\n",
    "\n",
    " # Normalize logits and spans to retrieve the answer\n",
    "start_ = np.exp(start_ - np.log(np.sum(np.exp(start_), axis=-1, keepdims=True)))\n",
    "end_ = np.exp(end_ - np.log(np.sum(np.exp(end_), axis=-1, keepdims=True)))\n",
    "\n",
    "# Compute the score of each tuple(start, end) to be the real answer\n",
    "outer = np.matmul(np.expand_dims(start_, -1), np.expand_dims(end_, 1))\n",
    "\n",
    "# Remove candidate with end < start and end - start > max_answer_len\n",
    "max_answer_len = 15\n",
    "candidates = np.tril(np.triu(outer), max_answer_len - 1)\n",
    "scores_flat = candidates.flatten()\n",
    "\n",
    "idx_sort = [np.argmax(scores_flat)]\n",
    "start, end = np.unravel_index(idx_sort, candidates.shape)[1:]\n",
    "end += 1\n",
    "score = candidates[0, start, end-1]\n",
    "start, end, score = start.item(), end.item(), score.item()\n",
    "\n",
    "\n",
    "print(tokenizer.decode(input_ids[start:end]).strip())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.3962889611721039, 'start': 47, 'end': 56, 'answer': 'grey blue'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "QA_input = {\n",
    "    'question': q1,\n",
    "    'context': context\n",
    "}\n",
    "\n",
    "nlp(QA_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask_question(q1, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading model from ml_model/valhalla/t5-small-qa-qg-hl\n",
      "[+] Loading completed\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer, model = ModelLoader(model_name='valhalla/t5-small-qa-qg-hl',\n",
    "                            model_directory=MODEL_PATH,\n",
    "                            tokenizer_loader=AutoTokenizer,\n",
    "                            model_loader=AutoModelForSeq2SeqLM,\n",
    "                            ).retrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"question: What answer to life? context: 42 is the answer to life, the universe and everything.\"\n",
    "\n",
    "features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> 42</s>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading model from ml_model/dslim/bert-base-NER\n",
      "[+] Loading completed\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_loader = ModelLoader(model_name=\"dslim/bert-base-NER\",\n",
    "                            model_directory=MODEL_PATH,\n",
    "                            tokenizer_loader=AutoTokenizer,\n",
    "                            model_loader=AutoModelForTokenClassification,\n",
    "                            )\n",
    "\n",
    "tokenizer, model = ner_loader.retrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "example = \"My name is Prayson and I live in Copenhagen. I work at NASA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER', 'score': 0.9993826746940613, 'word': 'P'},\n",
       " {'entity_group': 'PER', 'score': 0.8052563965320587, 'word': '##rayson'},\n",
       " {'entity_group': 'LOC', 'score': 0.9989917278289795, 'word': 'Copenhagen'},\n",
       " {'entity_group': 'ORG', 'score': 0.9983671307563782, 'word': 'NASA'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading model from ml_model/valhalla/t5-small-qa-qg-hl\n",
      "[+] Loading completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelLoader(model=ml_model/valhalla/t5-small-qa-qg-hl)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelLoader(model_name='valhalla/t5-small-qa-qg-hl',\n",
    "                            model_directory=MODEL_PATH,\n",
    "                            tokenizer_loader=AutoTokenizer,\n",
    "                            model_loader=AutoModelForSeq2SeqLM,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
